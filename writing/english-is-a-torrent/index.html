<html>
    <head>
        <link href="/style.css" rel="stylesheet" type="text/css"/>
        <link href="../style.css" rel="stylesheet" type="text/css"/>
        <script src="/script.js"></script>
        <title>English is a Torrent</title>
    </head>
    <body>
        <div id="transition-overlay"></div>
        <!--nav-->
        <div class="nav">
            <!--title-->
            <div id="header-container"></div>
            <script>
                fetch('/header.html')
                    .then(response => response.text())
                    .then(html => {
                        document.getElementById('header-container').innerHTML = html;
                    });
            </script>
        </div> <!--nav end-->
        <!--main-->
        <div class="writing-wrapper">
            <p><a href="../../index.html">&#8592 Back <i>(writing)</i></a></p>
            <div class="writing-body">
                <div class="title">
                    <h1>English is a Torrent</h1>
                    <p>Edited by Arete Xu</p>
                </div>
                <div class="section left">
                    <p>
                        English is hard; the rigid enforcement of “proper” English demands adherence from a system filled with contradictions. Why are “cough,” “though,” “through,” and “tough” all pronounced differently? Why does “knight” hide half its sounds? And what’s up with the deadly homophonic trio of “there,” “they’re,” and “their”? Why don’t the sounds and spellings in English line up? This isn’t aksidental—it emerges from the ongoing tenshun between power strukchures imposing standardizshun and communities activly rezisting these norms.
                    </p>
                    <p>
                        Historically, English has always had a voracious appetite for linguistic borrowing, collecting more and more words with sounds our writing system couldn’t represent. Our alphabet, composed of 26 letters, is stretched thin over 44 phonetic sounds, making writing English an <a href="https://en.wikipedia.org/wiki/Oulipo" target="_blank">Oulipian</a> exercise in representation. This lexicon borrowing brings orthographic ghosts from their languages of origin, forcing the English writing system to evolve away from the uniform ideal attempted by colonial Britain.
                    </p>
                    <div class="">
                        <img src="images/p2p.jpg" style="transform:scaleY(0.75)">
                        <p class="caption">
                            Peer to Peer network (Diagram from “Architectural model for Wireless Peer-to-Peer (WP2P) file sharing for ubiquitous mobile devices” (2009), Ayodeji Oluwatope et al.)
                        </p>
                    </div>
                    <p>
                        Today, digital spaces open up new possibilities for orthographic adaptations, deployed and distributed in something closer to guerrilla tactics rather than an organized effort. Our language network might seem like an open source project—a communal effort we all contribute to—but the absence of organized repositories, documentation, and governing maintainers who approve or reject changes makes it more like a torrent system. With no central server hosting definitive meanings, cultural understanding is seeded peer-to-peer across overlapping networks as users simultaneously download and upload fragments of meaning. This torrent-like quality means that no single source ever possesses the complete codebase of our language. Its inaccessibility creates spaces where communities can develop their own linguistic sovereignty away from dominant culture’s standardizing impulses.
                    </p>
                </div>
                <div class="section right">
                    <h2>Power Systems Shape Writing Systems</h2>
                    <p>
                        Writing systems have never been neutral reflections of language, but rather technologies wielded by those in power. From the Roman Empire’s spread of the Latin script (also known as the Roman script, used interchangeably in this article), to the Cyrllic-based scripts that replaced Arabic scripts in Central Asia, powerful states have long recognized that controlling how people write is nearly as important as controlling what they write.
                    </p>
                    <p>
                        The standardization and policing of written language is achieved through orthography, the system governing how a language is represented in written form. It’s a broad blueprint that determines which symbols correspond to which sounds, how words are segmented, and how grammatical features are marked on the page. The term derives from Greek orthos (correct) and graphein (to write)—its normative function embedded from the very beginning. Sociolinguists assert that “orthography is one of the key sites where the very notion of “standard language” is policed.”<sup>[1]</sup> This policing of correctness is often systemic and institutionalized, from academic style guides to publishing houses, to classrooms and standardized testing.
                    </p>
                    <p>
                        With the imposition of orthography comes potential for faulty transliteration between languages. Romanization systems map Latin characters onto diverse phonological systems with little consistency across languages. Some letter combinations suddenly contradict expectations, producing completely different sounds depending on linguistic context. Ironically, the Latin alphabet wasn’t even designed for English. It was created, well, for Latin. Nevertheless, the Latin alphabet has become a “global script”—for roughly 70% of languages worldwide—pressed into service for languages it was never designed to accommodate.<sup>[2]</sup> Yet this ubiquity creates a dilemma: while the Roman script makes languages more legible in a global audience, it simultaneously flattens their rich phonetic landscapes. Tones, glottal stops, and the musical rhythms of speech dissolve into silent letters. This standardization isn’t neutral, instead privileging certain ways of speaking while erasing marginalized communities. But what’s the most fascinating, is how communities are resisting homogenization by reclaiming the Latin script with our new technologies.
                    </p>
                    <p>
                        With just the clicks of our fingers now, digital communication creates turbulent spaces where language is sampled, remixed, and transformed through community usage rather than institutional decree. When Jamaican Creole speakers write “yaad” for “yard” or “wid” for “with” in their texts, they're making deliberate choices to preserve vernacular authenticity and establish community boundaries, holding language close from the scripted pull-away of autocorrect. Similarly, in Singapore, Singlish orthography flourishes online despite the government’s official Speak Good English Movement.<sup>[3]</sup> Discourse particles like “lah,”, “lor,” and “leh” and distinctive spellings like “liddat” for “like that” become assertions of a multilingual identity that resists both standard English dominance and language purism policies.<sup>[4]</sup> I try to play a small part in the quiet rebellion against linguistic homogenization too by stubbornly keeping my keyboard set to Canadian English, deploying “colour” like a small flag of resistance against the wave of red squiggles. The “I'm not American!” signal, while definitely annoying enough for a lot of my friends, immediately creates a connection with fellow Commonwealth spellers; a single “u” becomes an unlikely but effective community marker.
                    </p>
                </div>
                <div class="section">
                    <h2>The Internet was Built for English</h2>
                    <p>
                        Could our resistance against the red squiggles be part of a bigger struggle against the internet’s systematic enforcement for language standardization? The Internet’s climactic rise made a thrilling promise of liberating communication. But it came with a caveat. Digital orthography, language transcription on the internet, was designed with architectural decisions that privileged language communication in English. Born in the labs of the American defense establishment in the 1960s, the internet’s foundation rests on ASCII (American Standard Code for Information Interchange)—a character encoding standard supporting only 128 characters, comfortably adequate for English but woefully insufficient for most of the world’s writing systems.
                    </p>
                    <div>
                        <img src="images/ascii.png">
                        <p class="caption">
                            (ASCII chart from MIL-STD-188-100, 1972)https://en.m.wikipedia.org/wiki/File:USASCII_code_chart.png
                        </p>
                    </div>
                    <p>
                        ASCII’s limitations reflected the practical priorities of its American developers who were primarily concerned with standardizing computer communication in English. The resulting character encoding standard created an electronic medium that inadvertently privileged Latin-based scripts while presenting significant challenges for other writing systems. As the internet expanded globally throughout the 1980s and 1990s, the Latin character set became the internet’s de facto linguistic currency—a medium of exchange that non-English users were forced to adopt despite its poor exchange rate for their native scripts. This technical foundation highlighted previously blurred boundaries between English as a language, English orthography, and the Latin character set itself, as users worldwide had to consciously navigate these distinctions to participate in digital communication. This created a peculiar scenario where billions of people, as they connected to the rapidly expanding internet, found their native scripts unsupported by the very technology promising to connect humanity.
                    </p>
                    <p>
                        Faced with this digital “straightjacket”, language communities worldwide developed workarounds that transformed constraints into creative opportunity. Arabic speakers repurposed numerals as visual stand-ins for sounds without Roman equivalents, with <3> representing <ع> and <7> standing in for <خ>. Greek users similarly employed visual equivalence, substituting <w> for <ω> and <8> for <θ>, much to the horror of the Academy of Athens, who promptly declared such adaptations are “a threat to the Greek language.”<sup>[5]</sup> What language purists and institutional authorities often label a “language crisis” might better be understood as the birth pangs of new digital language conventions. Languages with Cyrillic alphabets underwent similar transformations—eliminating diacritic marks, substituting vowels and modifying consonants—not as a surrender to English dominance but as tactical adaptation. Even as Unicode, an expanded standard for online text encoding, eventually widened our digital character set, these early adaptations created a new set of needs in digital orthography: a distributed, community-driven approach to digital communication, something that could maintain distinctive linguistic identity with limited tools.
                    </p>
                </div>
            </div> <!--project-images end-->
        </div> <!--main end-->
        <script>
            function stagger(){
                const h2 = document.querySelectorAll('h2');
                
                h2.forEach(h2 => {
                    const text = h2.textContent;
                    h2.innerHTML = '';
                    
                    for(let i = 0; i < text.length; i++){
                        const randomPos = Math.floor(Math.random() * 5) + 1;
                        const char = text[i];
                        const span = document.createElement('span');
                        span.textContent = char;
                        span.className = 'char-span';
                        
                        span.style.transform = 'translateY(' + randomPos + 'px)';
                        
                        h2.appendChild(span);
                    }
                });
            }
            
            document.addEventListener('DOMContentLoaded', stagger);
        </script>
    </body>
</html>